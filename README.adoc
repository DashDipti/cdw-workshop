= Cloudera Data Warehouse -Workshop Student Guide

'''

[Underconstruction] Version : 1.0.0 `27th March 2023` +

'''

== Introduction

Working for GE Aircraft Engine, the company wants to increase competitive advantage in two key ways: +
(1) Engineer better, more fault tolerant aircraft engines. +
(2) Be proactive in predictive maintenance on engines, and faster discovery-to-fix in new engine designs. +

This will be a three phase plan: +
*(1) Phase One:*  Understand how our current engines contribute to airline flight delays and fix for future engines. +
*(2) Phase Two:*  Implement an ongoing reporting service to support ongoing engineering efforts to continuously improve engines based on delay data. +
*(3) Phase Three:*  Move to real-time analysis to fix things before they break both in engines already sold, and in new engine designs. +

To do this, we’re going to build a data warehouse & data lakehouse to create reports that engineers can use to improve our engines.  The following people will get to work: +

== High-Level Steps

Below are the high-level steps for what we will be doing in the workshop. +
*(1) [Step 1 & 2]:* General introduction to CDW to get ourselves oriented for the workshop.  +
    . As an Admin: Create and enable the BI analyst team with a Virtual Warehouse
    . As a BI Analyst:  Get familiar with CDW on CDP, and set up our first VW to start working
    . As a BI Analyst:  Wrangle our first set of data - sent to us as a series of .csv files exported from “somewhere else”
    . As an Admin: Monitor the VW and watch as it scales up and down, suspends, etc.
    . As a BI Analyst:  Start digging into the data - looking for “needle in a haystack” - running a complex query that will find which engines seem to be correlated to airplane delays for any reason.

*(2) [Step 3]:* CDF workflow and run it to ingest data into S3. +
    . As a BI Analyst: Start curating data and building a data lakehouse to improve quality by tweaking data, performance by optimizing schema structures, and ensure reliability and trustworthyness of the data through snapshots, time travel, and rollback
    . Create Hive ACID tables and tweak data for consistency (ex: airline name changes - ensure reporting is consistent with the new name to avoid end user confusion, a new airline joins our customer list, make sure they’re tracked for future data collection, etc.)
    . Migrate Tables to Iceberg (We want snapshot and rollback)
    . Create new Iceberg tables (we want partitioning)

*(3) [Step 4]:* Iceberg Table using Cloudera Data Warehouse (CDW/Hue). +
*(4) [Step 5,6 & 7]:* CDE job and run it to ingest data into iceberg table. +

== Pre-requisites

. Laptop with a supported OS (Windows 7 not supported) or MacBook.
. A modern browser - Google Chrome (IE, Firefox, Safari not supported).
. Wi-Fi Internet connection.
. Git installed.



== Step 1: XXXXX

=== Step 1(a): Query Iceberg Tables in Hue and Cloudera Data Visualization

=== Step 1(a): For Reading only (Optional): Iceberg Architecture



=== Notes


*Note*: Higlight
[,sql]
----

CREATE TABLE IF NOT EXISTS <user>_stocks.stock_intraday_1min (
  interv STRING,
  output_size STRING,
  time_zone STRING,
  open DECIMAL(8,4),
  high DECIMAL(8,4),
  low DECIMAL(8,4),
  close DECIMAL(8,4),
  volume BIGINT)
PARTITIONED BY (
  ticker STRING,
  last_refreshed string,
  refreshed_at string)
STORED AS iceberg;
----

____
(user)_stock_dataflow +
____

image:images/step5/8.PNG[]  +

Let parameters be the default ones. Click `Next`.

