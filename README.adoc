= [Underconstruction] 
image:images/misc/UC.PNG[]

image:images/step0/banner-impact24.PNG[] 
= Cloudera Data Warehouse - Workshop Student Guide

'''

Version : 1.0.0 `27th March 2023` +

'''
== Pre-requisites

. Laptop with a supported OS (Windows 7 not supported) or MacBook.
. A modern browser - Google Chrome (IE, Firefox, Safari not supported).


== Preface

Working for GE Aircraft Engine, the company wants to increase competitive advantage in two key ways: +
(1) Engineer better, more fault tolerant aircraft engines. +
(2) Be proactive in predictive maintenance on engines, and faster discovery-to-fix in new engine designs. +

This will be a three phase plan: +
*(1) Phase One:*  Understand how our current engines contribute to airline flight delays and fix for future engines. +
*(2) Phase Two:*  Implement an ongoing reporting service to support ongoing engineering efforts to continuously improve engines based on delay data. +
*(3) Phase Three:*  Move to real-time analysis to fix things before they break both in engines already sold, and in new engine designs. +

To do this, we’re going to build a data warehouse & data lakehouse to create reports that engineers can use to improve our engines.  The following people will get to work: +


We will dive into this Scenario to show Cloudera Data Warehouse (CDW) is used to enable GE Aircraft to gain competitive advantage - and at the same time it highlights the performance and automation capabilities that help ensure performance is maintained while controlling costs. +

The Hands On Labs will take you through how to use the Cloudera Data Warehouse service to quickly explore raw data, create curated versions of the data for simple reporting and dashboarding, and then scale up usage of the curated data by exposing it to more users. +

*ER - Diagram of the data* +
*(1) Fact Table:*  flights (86M rows) +
*(2) Dimension Table:*  airlines (1.5k rows), airports (3.3k rows) and planes (5k rows) +

== High-Level Steps

Below are the high-level steps for what we will be doing in the workshop. +
*(1) [Step 1 & 2]:* General introduction to CDW to get ourselves oriented for the workshop.  +

    (a) As an Admin: Create and enable the BI analyst team with a Virtual Warehouse.
    (b) As a BI Analyst:  Get familiar with CDW on CDP, and set up our first VW to start working.
    (c) As a BI Analyst:  Wrangle our first set of data - sent to us as a series of .csv files exported from “somewhere else”.
    (d) As an Admin: Monitor the VW and watch as it scales up and down, suspends, etc.
    (e) As a BI Analyst:  Start digging into the data - looking for “needle in a haystack” - running a complex query that will find which engines seem to be correlated to airplane delays for any reason.

*(2) [Step 3]:* Set it up. +

    (a) As an Admin: Create and enable the BI analyst team with a Virtual Warehouse.
    (b) As a BI Analyst:  Get familiar with CDW on CDP, and set up our first VW to start working.
    (c) As a BI Analyst:  Wrangle our first set of data - sent to us as a series of .csv files exported from “somewhere else”.
    (d) As an Admin: Monitor the VW and watch as it scales up and down, suspends, etc.
    (e) As a BI Analyst:  Start digging into the data - looking for “needle in a haystack” - running a complex query that will find which engines seem to be correlated to airplane delays for any reason.

*(3) [Step 4]:* Making it better. +

    (a) As a BI Analyst: Start curating data and building a data lakehouse to improve quality by tweaking data, performance by optimizing schema structures, and ensure reliability and trustworthyness of the data through snapshots, time travel, and rollback.
    (b) Create Hive ACID tables and tweak data for consistency (ex: airline name changes - ensure reporting is consistent with the new name to avoid end user confusion, a new airline joins our customer list, make sure they’re tracked for future data collection, etc..).
    (c) Migrate Tables to Iceberg (We want snapshot and rollback).
    (d) Create new Iceberg tables (we want partitioning).

*(4) [Step 5]:* Optimizing for production. +

    (a) Loading more data - change partitioning to maintain performance  (NOTE:  Ongoing ELT = CDE?).
    (b) Bad data is loaded - use time travel to detect, and rollback to resolve.
    (c) Introduce materialized views to support scaling to 1000’s of simultaneous users.
    (d) As an admin:  Monitor, report, kill queries that run amock, etc.
    
*(5) [Step 6]:* Security & Governance. +

    (a) Check on the lineage to enable governance/audit.
    (b) Row level security to make sure only relevant party can see data.


== Introduction

. Laptop with a supported OS (Windows 7 not supported) or MacBook.
. A modern browser - Google Chrome (IE, Firefox, Safari not supported).
. Wi-Fi Internet connection.


== Step 1: XXXXX

=== Step 1(a): Query Iceberg Tables in Hue and Cloudera Data Visualization

=== Step 1(a): For Reading only (Optional): Iceberg Architecture



=== Notes


*Note*: Higlight
[,sql]
----

CREATE TABLE IF NOT EXISTS <user>_stocks.stock_intraday_1min (
  interv STRING,
  output_size STRING,
  time_zone STRING,
  open DECIMAL(8,4),
  high DECIMAL(8,4),
  low DECIMAL(8,4),
  close DECIMAL(8,4),
  volume BIGINT)
PARTITIONED BY (
  ticker STRING,
  last_refreshed string,
  refreshed_at string)
STORED AS iceberg;
----

____
(user)_stock_dataflow +
____

image:images/step5/8.PNG[]  +

Let parameters be the default ones. Click `Next`.

